{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "wn=nltk.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "train_data=pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text=\"\".join([i.lower() for i in text if i not in string.punctuation])\n",
    "    #text=re.sub('@\\w+','',text)\n",
    "    tokens=[i for i in re.split('\\W+',text) if  i !='']\n",
    "    tokens=[i for i in tokens if i not in stopwords]\n",
    "    return tokens\n",
    "    \n",
    "    \n",
    "def clean_text2(text):\n",
    "    text=\"\".join([i.lower() for i in text if i not in string.punctuation]) # remove punctuation\n",
    "    #text=re.sub('@\\w+','',text) #remove words starting with @ these are usually usernames\n",
    "    text=re.sub('[0-9]+','',text) #remove everything starting with numbers\n",
    "    text=re.sub(r'http\\S+', '', text)\n",
    "    tokens=[i for i in re.split('\\W+',text) if  i !=''] \n",
    "    tokens=[i for i in tokens if i not in stopwords] # remove stopwords\n",
    "    text=\" \".join([wn.lemmatize(word) for word in tokens])\n",
    "    return text\n",
    "\n",
    "\n",
    "def removeusername(x):\n",
    "    return re.sub('@\\w+','',x)\n",
    "\n",
    "def removeemoji(x):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', x) # no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Rmusername']=train_data['text'].apply(lambda x : removeusername(x))\n",
    "\n",
    "train_data['Remoji']=train_data['Rmusername'].apply(lambda x : removeemoji(x))\n",
    "\n",
    "train_data['Cleaned_text']=train_data['Remoji'].apply(lambda x :clean_text2(x))\n",
    "\n",
    "DV=pd.DataFrame(train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Rmusername</th>\n",
       "      <th>Remoji</th>\n",
       "      <th>Cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>two giant crane holding bridge collapse nearby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>The out of control wild fires in California ...</td>\n",
       "      <td>The out of control wild fires in California ...</td>\n",
       "      <td>control wild fire california even northern par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>utckm volcano hawaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>police investigating ebike collided car little...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>latest home razed northern california wildfire...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                             Rmusername  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...   \n",
       "1                Forest fire near La Ronge Sask. Canada   \n",
       "2     All residents asked to 'shelter in place' are ...   \n",
       "3     13,000 people receive #wildfires evacuation or...   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...   \n",
       "...                                                 ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...   \n",
       "7609    The out of control wild fires in California ...   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "7611  Police investigating after an e-bike collided ...   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "                                                 Remoji  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...   \n",
       "1                Forest fire near La Ronge Sask. Canada   \n",
       "2     All residents asked to 'shelter in place' are ...   \n",
       "3     13,000 people receive #wildfires evacuation or...   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...   \n",
       "...                                                 ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...   \n",
       "7609    The out of control wild fires in California ...   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "7611  Police investigating after an e-bike collided ...   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "                                           Cleaned_text  \n",
       "0            deed reason earthquake may allah forgive u  \n",
       "1                 forest fire near la ronge sask canada  \n",
       "2     resident asked shelter place notified officer ...  \n",
       "3     people receive wildfire evacuation order calif...  \n",
       "4     got sent photo ruby alaska smoke wildfire pour...  \n",
       "...                                                 ...  \n",
       "7608  two giant crane holding bridge collapse nearby...  \n",
       "7609  control wild fire california even northern par...  \n",
       "7610                               utckm volcano hawaii  \n",
       "7611  police investigating ebike collided car little...  \n",
       "7612  latest home razed northern california wildfire...  \n",
       "\n",
       "[7613 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 57104)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(ngram_range=(1,2))\n",
    "cv1=cv.fit(train_data['Cleaned_text'])\n",
    "X_CV=cv1.transform(train_data['Cleaned_text'])\n",
    "print(X_CV.shape)\n",
    "#print(cv.get_feature_names())\n",
    "Xcv_train=pd.DataFrame(X_CV.toarray())\n",
    "Xcv_train.columns=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['amp', 'fire', 'get', 'im', 'like', 'new', 'one', 'via'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xcv_train.columns[Xcv_train.sum()>200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aa ayyo', 'aa battery', 'aa near', 'aaaa', 'aaaa ok', 'aaaaaaallll',\n",
       "       'aaaaaaallll ûªm', 'aaaaaand', 'aaaaaand there', 'aaarrrgghhh',\n",
       "       ...\n",
       "       'ûóher', 'ûóher upper', 'ûókody', 'ûókody vine', 'ûónegligence',\n",
       "       'ûónegligence firework', 'ûótech', 'ûótech business', 'ûówe',\n",
       "       'ûówe work'],\n",
       "      dtype='object', length=46166)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xcv_train.columns[Xcv_train.sum()==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 10929)\n"
     ]
    }
   ],
   "source": [
    "Xcv_train.drop(Xcv_train.columns[Xcv_train.sum()==1],axis=1,inplace=True) # Removing words low frequency words\n",
    "Xcv_train.drop(['amp', 'dont', 'get', 'im', 'like', 'new', 'one','people', 'via'],axis=1,inplace=True) #removing high frequency words\n",
    "print(Xcv_train.shape)\n",
    "#print(Xcv_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xcv_train, DV, test_size=0.2,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "param={\n",
    "    'max_depth':[10,20,50,100],\n",
    "    'n_estimators':[50,100,150]\n",
    "    \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "clf = RandomizedSearchCV(rf, param, random_state=0)\n",
    "rf_model=clf.fit(X_train,y_train.values.ravel())\n",
    "randomforest_model2=pd.DataFrame(clf.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.961664</td>\n",
       "      <td>0.259803</td>\n",
       "      <td>0.170313</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 150, 'max_depth': 100}</td>\n",
       "      <td>0.779146</td>\n",
       "      <td>0.749589</td>\n",
       "      <td>0.765189</td>\n",
       "      <td>0.774220</td>\n",
       "      <td>0.760263</td>\n",
       "      <td>0.765681</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.274104</td>\n",
       "      <td>0.243921</td>\n",
       "      <td>0.123703</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 100}</td>\n",
       "      <td>0.775041</td>\n",
       "      <td>0.743842</td>\n",
       "      <td>0.768473</td>\n",
       "      <td>0.774220</td>\n",
       "      <td>0.760263</td>\n",
       "      <td>0.764368</td>\n",
       "      <td>0.011541</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.174195</td>\n",
       "      <td>0.866907</td>\n",
       "      <td>0.083299</td>\n",
       "      <td>0.008680</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 100}</td>\n",
       "      <td>0.777504</td>\n",
       "      <td>0.747947</td>\n",
       "      <td>0.766010</td>\n",
       "      <td>0.771757</td>\n",
       "      <td>0.756979</td>\n",
       "      <td>0.764039</td>\n",
       "      <td>0.010522</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.310394</td>\n",
       "      <td>0.610912</td>\n",
       "      <td>0.114521</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 150, 'max_depth': 50}</td>\n",
       "      <td>0.746305</td>\n",
       "      <td>0.728243</td>\n",
       "      <td>0.737274</td>\n",
       "      <td>0.743842</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.738752</td>\n",
       "      <td>0.006261</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.087030</td>\n",
       "      <td>0.096608</td>\n",
       "      <td>0.087350</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 50}</td>\n",
       "      <td>0.738916</td>\n",
       "      <td>0.727422</td>\n",
       "      <td>0.745484</td>\n",
       "      <td>0.738916</td>\n",
       "      <td>0.740558</td>\n",
       "      <td>0.738259</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.162378</td>\n",
       "      <td>0.157070</td>\n",
       "      <td>0.061582</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 50}</td>\n",
       "      <td>0.743842</td>\n",
       "      <td>0.716749</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.747947</td>\n",
       "      <td>0.732348</td>\n",
       "      <td>0.735796</td>\n",
       "      <td>0.010885</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.893430</td>\n",
       "      <td>0.228324</td>\n",
       "      <td>0.051733</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 20}</td>\n",
       "      <td>0.695402</td>\n",
       "      <td>0.667488</td>\n",
       "      <td>0.695402</td>\n",
       "      <td>0.678982</td>\n",
       "      <td>0.697865</td>\n",
       "      <td>0.687028</td>\n",
       "      <td>0.011868</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.191709</td>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.063609</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 20}</td>\n",
       "      <td>0.691297</td>\n",
       "      <td>0.661741</td>\n",
       "      <td>0.698686</td>\n",
       "      <td>0.671593</td>\n",
       "      <td>0.697865</td>\n",
       "      <td>0.684236</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.710839</td>\n",
       "      <td>0.079857</td>\n",
       "      <td>0.065977</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 150, 'max_depth': 10}</td>\n",
       "      <td>0.651888</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.660920</td>\n",
       "      <td>0.633826</td>\n",
       "      <td>0.662562</td>\n",
       "      <td>0.648276</td>\n",
       "      <td>0.012998</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.823422</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.055458</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 10}</td>\n",
       "      <td>0.650246</td>\n",
       "      <td>0.633005</td>\n",
       "      <td>0.656814</td>\n",
       "      <td>0.626437</td>\n",
       "      <td>0.649425</td>\n",
       "      <td>0.643186</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1      40.961664      0.259803         0.170313        0.002575   \n",
       "3      27.274104      0.243921         0.123703        0.001182   \n",
       "8      14.174195      0.866907         0.083299        0.008680   \n",
       "5      24.310394      0.610912         0.114521        0.000263   \n",
       "7      16.087030      0.096608         0.087350        0.000720   \n",
       "0       8.162378      0.157070         0.061582        0.000483   \n",
       "9       3.893430      0.228324         0.051733        0.001931   \n",
       "2       7.191709      0.010842         0.063609        0.000413   \n",
       "4       5.710839      0.079857         0.065977        0.002301   \n",
       "6       3.823422      0.002383         0.055458        0.000831   \n",
       "\n",
       "  param_n_estimators param_max_depth                                   params  \\\n",
       "1                150             100  {'n_estimators': 150, 'max_depth': 100}   \n",
       "3                100             100  {'n_estimators': 100, 'max_depth': 100}   \n",
       "8                 50             100   {'n_estimators': 50, 'max_depth': 100}   \n",
       "5                150              50   {'n_estimators': 150, 'max_depth': 50}   \n",
       "7                100              50   {'n_estimators': 100, 'max_depth': 50}   \n",
       "0                 50              50    {'n_estimators': 50, 'max_depth': 50}   \n",
       "9                 50              20    {'n_estimators': 50, 'max_depth': 20}   \n",
       "2                100              20   {'n_estimators': 100, 'max_depth': 20}   \n",
       "4                150              10   {'n_estimators': 150, 'max_depth': 10}   \n",
       "6                100              10   {'n_estimators': 100, 'max_depth': 10}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "1           0.779146           0.749589           0.765189           0.774220   \n",
       "3           0.775041           0.743842           0.768473           0.774220   \n",
       "8           0.777504           0.747947           0.766010           0.771757   \n",
       "5           0.746305           0.728243           0.737274           0.743842   \n",
       "7           0.738916           0.727422           0.745484           0.738916   \n",
       "0           0.743842           0.716749           0.738095           0.747947   \n",
       "9           0.695402           0.667488           0.695402           0.678982   \n",
       "2           0.691297           0.661741           0.698686           0.671593   \n",
       "4           0.651888           0.632184           0.660920           0.633826   \n",
       "6           0.650246           0.633005           0.656814           0.626437   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "1           0.760263         0.765681        0.010419                1  \n",
       "3           0.760263         0.764368        0.011541                2  \n",
       "8           0.756979         0.764039        0.010522                3  \n",
       "5           0.738095         0.738752        0.006261                4  \n",
       "7           0.740558         0.738259        0.005930                5  \n",
       "0           0.732348         0.735796        0.010885                6  \n",
       "9           0.697865         0.687028        0.011868                7  \n",
       "2           0.697865         0.684236        0.014902                8  \n",
       "4           0.662562         0.648276        0.012998                9  \n",
       "6           0.649425         0.643186        0.011478               10  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforest_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "for i in [10,20,50]:\n",
    "    for j in [50,100,150]:\n",
    "        xgb=XGBClassifier(max_depth=i,n_estimators=j)\n",
    "        xgbmodel=xgb.fit(X_train,y_train.values.ravel())\n",
    "        \n",
    "        y_pred=xgbmodel.predict(X_test)\n",
    "        \n",
    "        print(\"Max depth: {}, N_estimator: {}, Recall : {}, Precision : {}, Accuracy : {}, F1-score : {}\".format(i,j,round(recall_score(y_test,y_pred),3),round(precision_score(y_test,y_pred),3),round(accuracy_score(y_test,y_pred),3),round(f1_score(y_test,y_pred),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth: 20, N_estimator: 50, Recall : 0.666, Precision : 0.794, Accuracy : 0.788, F1-score : 0.724\n"
     ]
    }
   ],
   "source": [
    "xgb=XGBClassifier(max_depth=50,n_estimators=50)\n",
    "xgbmodel=xgb.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "y_pred=xgbmodel.predict(X_test)\n",
    "print(\"Max depth: {}, N_estimator: {}, Recall : {}, Precision : {}, Accuracy : {}, F1-score : {}\".format(20,50,round(recall_score(y_test,y_pred),3),round(precision_score(y_test,y_pred),3),round(accuracy_score(y_test,y_pred),3),round(f1_score(y_test,y_pred),3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 57104)\n",
      "(3263, 57104)\n",
      "(3263, 10929)\n"
     ]
    }
   ],
   "source": [
    "test_data=pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "test_data['Rmusername']=test_data['text'].apply(lambda x : removeusername(x))\n",
    "\n",
    "test_data['Remoji']=test_data['Rmusername'].apply(lambda x : removeemoji(x))\n",
    "\n",
    "test_data['Cleaned_text']=test_data['Remoji'].apply(lambda x :clean_text2(x))\n",
    "\n",
    "\n",
    "X_CV_test=cv1.transform(test_data['Cleaned_text'])\n",
    "print(X_CV_test.shape)\n",
    "\n",
    "#print(cv1.get_feature_names())\n",
    "\n",
    "\n",
    "X_CV_test=pd.DataFrame(X_CV_test.toarray())\n",
    "X_CV_test.columns=cv1.get_feature_names()\n",
    "print(X_CV_test.shape)\n",
    "\n",
    "X_CV_test=X_CV_test[Xcv_train.columns]\n",
    "print(X_CV_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=xgbmodel.predict(X_CV_test)\n",
    "\n",
    "resultdic=dict()\n",
    "resultdic['id']=test_data['id'].values\n",
    "resultdic['text']=test_data['text'].values\n",
    "resultdic['Prediction']=y_pred\n",
    "final_result=pd.DataFrame(resultdic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>What a nice hat?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  Prediction\n",
       "1   2  Heard about #earthquake is different cities, s...           1\n",
       "2   3  there is a forest fire at spot pond, geese are...           1\n",
       "3   9           Apocalypse lighting. #Spokane #wildfires           1\n",
       "4  11      Typhoon Soudelor kills 28 in China and Taiwan           1\n",
       "5  12                 We're shaking...It's an earthquake           1\n",
       "6  21  They'd probably still show more life than Arse...           0\n",
       "7  22                                  Hey! How are you?           0\n",
       "8  27                                   What a nice hat?           0\n",
       "9  29                                          Fuck off!           0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv(\"testpredictions.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
